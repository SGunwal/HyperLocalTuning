{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za6clRosUoql"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def generate_elastic_net_data(n_samples=100, n_features=50, n_informative=10, n_targets=1,\n",
        "                              noise=0.1, effective_rank=None, tail_strength=0.5, random_state=42,\n",
        "                              corr_strength=0.8, sparsity=0.8):\n",
        "    \"\"\"\n",
        "    Generates a synthetic dataset suitable for Elastic Net regularization.\n",
        "\n",
        "    Parameters:\n",
        "    - n_samples: int, default=100\n",
        "        The number of samples.\n",
        "    - n_features: int, default=50\n",
        "        The total number of features.\n",
        "    - n_informative: int, default=10\n",
        "        The number of informative features, i.e., features used to build the linear model.\n",
        "    - n_targets: int, default=1\n",
        "        The number of regression targets, i.e., the dimension of the Y output.\n",
        "    - noise: float, default=0.1\n",
        "        The standard deviation of the gaussian noise applied to the output.\n",
        "    - effective_rank: int or None, default=None\n",
        "        If not None, the approximate number of singular vectors required to explain most of the data.\n",
        "        Useful for creating correlated features.\n",
        "    - tail_strength: float, default=0.5\n",
        "        The relative importance of the fat noisy tail of the singular values profile if effective_rank is not None.\n",
        "    - random_state: int or None, default=None\n",
        "        Determines random number generation for dataset creation.\n",
        "    - corr_strength: float, default=0.8\n",
        "        The correlation strength between informative features. Should be between 0 and 1.\n",
        "    - sparsity: float, default=0.8\n",
        "        The fraction of coefficients that are zero in the underlying model. Should be between 0 and 1.\n",
        "\n",
        "    Returns:\n",
        "    - X: numpy array of shape (n_samples, n_features)\n",
        "        The input samples.\n",
        "    - y: numpy array of shape (n_samples,) or (n_samples, n_targets)\n",
        "        The output values.\n",
        "    - coef: numpy array of shape (n_features,)\n",
        "        The true coefficients used to generate the data.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "    # Generate the base regression data\n",
        "    X, y, coef = make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_informative,\n",
        "                                 n_targets=n_targets, noise=noise, effective_rank=effective_rank,\n",
        "                                 tail_strength=tail_strength, coef=True, random_state=random_state)\n",
        "\n",
        "    # Add correlation between features by creating linear combinations of a few base features\n",
        "    if corr_strength > 0:\n",
        "        n_correlated_features = int(n_features * corr_strength)\n",
        "        base = np.random.randn(n_samples, n_correlated_features)\n",
        "        correlated_features = np.dot(base, np.random.randn(n_correlated_features, n_features))\n",
        "        X[:, :n_features] += correlated_features[:, :n_features]\n",
        "\n",
        "    # Introduce sparsity by zeroing out some coefficients\n",
        "    zero_indices = np.random.choice(np.arange(n_features), size=int(sparsity * n_features), replace=False)\n",
        "    coef[zero_indices] = 0\n",
        "\n",
        "    # Adjust output with the true (sparse) coefficients\n",
        "    y = np.dot(X, coef) + noise * np.random.randn(n_samples)\n",
        "\n",
        "    return X, y, coef\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Set size data\n",
        "number_of_samples  = 10000\n",
        "number_of_features = 100\n",
        "\n",
        "# Automatic setting for other characteristics\n",
        "fraction_of_informative_features = 0.1\n",
        "number_of_informative_features   = int(fraction_of_informative_features * number_of_features)\n",
        "effective_matrix_rank            = int(number_of_features*0.5)\n",
        "\n",
        "X, y, coef = generate_elastic_net_data(n_samples=number_of_samples, n_features=number_of_features, n_informative=number_of_informative_features,\n",
        "                                       noise=0.5, effective_rank=effective_matrix_rank, corr_strength=0.7, tail_strength=0.5, sparsity=0.7, random_state=42)\n",
        "\n",
        "# Convert to DataFrame for better handling (optional)\n",
        "df_X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
        "df_y = pd.Series(y, name=\"target\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_dataset(df_X, df_y, num_test=10000, validation_split=0.2, random_state=123):\n",
        "    \"\"\"\n",
        "    Splits the dataset into training, validation, and test sets.\n",
        "    \"\"\"\n",
        "\n",
        "    # Split out the test set\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        df_X, df_y, test_size=num_test, random_state=random_state)\n",
        "\n",
        "    # Split the remaining data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=validation_split, random_state=random_state)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Example usage:\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(\n",
        "    df_X, df_y, num_test=2000, validation_split=0.2, random_state=123)\n",
        "\n",
        "# Check the sizes of the splits\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2paV7OmUuHl",
        "outputId": "ef3305d7-caed-4711-cd85-5c098d0ff196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 6400 samples\n",
            "Validation set: 1600 samples\n",
            "Test set: 2000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "print( X_train.shape, X_val.shape, y_train.shape, y_val.shape, X_test.shape, y_test.shape  )\n",
        "\n",
        "full_data = { \"x_train\": X_train, \"x_val\": X_val, \"x_test\": X_test, \"y_train\": y_train, \"y_val\": y_val, \"y_test\": y_test }\n",
        "\n",
        "with open('/content/drive/MyDrive/HyperLocal_Tuning/Regression_HLS/Simulation_Dataset/simulation_dataset.pickle', 'wb') as handle:\n",
        "    pickle.dump( full_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtsYefgNVLKf",
        "outputId": "c73e3e0d-d977-4282-e4ca-51a9001f67b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6400, 100) (1600, 100) (6400,) (1600,) (2000, 100) (2000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aEHekourVT4n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}