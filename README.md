# HyperLocalTuning

We introduce a linear programming-based hyper local search approach for hyperparameter tuning of machine learning models. This approach fine-tunes continuous hyperparameters and model parameters through a linear program, enhancing model generalization in the vicinity of an initial model. The proposed method converts hyperparameter optimization into a bilevel program and identifies a descent direction to improve validation loss. Results from the paper show that this approach improves model quality in both validation and test performance.

---

## Directions for Running the Elastic Net Regression with Hyper Local Search

**Note**: Current directions are only available for running the code as a project in Visual Studio Code (VSCode). It should work on other editors as well, but the code has been developed specifically in VSCode. We will update a Jupyter notebook with detailed steps later.

### Overview

- The `datasets` folder contains the dataset used for the experiment.
- The `outputs` folder contains the intermediate and final results from running the experiments. The final outputs are stored as an Excel file inside the `results_summary` folder. This file contains the losses from Optuna training and improved losses using hyper local search, for different sampling methods. For each, the mean and 95% confidence interval values are provided. The files in the `outputs` folder are the exact outputs reported in the paper. A backup of these results is stored in `OUTPUT_BACKUPS`.

- The `utils` folder contains helper functions, Python files to install necessary libraries, and a `versions.txt` file that lists all the packages/libraries and their versions. This file is automatically generated when you run `installs.py`.

- The `optuna_training` folder contains a single Python file for training Optuna models for a given setting. Running this file independently will train the models and save the trained model weights, biases, hyperparameters, and runtime information. The final part of this file is commented out to allow the file to be executed independently while also being accessed by the main module for hyper local search: `HLS_one_direction_search.py`.

- The `hyper_local_search` folder contains the main code for tuning the model. The `curvature_information.py` file computes the gradients and necessary Hessian terms. The `loss_movement_plots.py` file plots the improvements in loss functions from Optuna training and local tuning with our method (though this module is currently not in use). The `HLS_one_direction_search.py` file contains the main functions for hyper local direction search, Golden Section Search (GSS), and analysis of the final outputs.

---

### Steps to Set Up and Run the Model

1. Set up the project in an editor such as Visual Studio Code (VSCode).
   
2. Run the files `utils/installs.py` and `utils/imports.py`. These will import the necessary libraries and packages. You can reference `versions.txt` for the specific versions.

3. Go to the main project file `HLS_one_direction_search.py` and modify the parameters as described below:

   3.1. **Set `OPTUNA_TRIALS` and `SIMULATIONS`**: These control the number of epochs in Optuna training and the number of times the experiments are repeated (both for Optuna training and the local tuning phase).

   3.2. **Directories**: The directories are automatically set to the current project folder. Please do not modify these directories.

   3.3. **Optuna Model Settings**: Check the `optuna_model_settings` dictionary, which contains the settings for Optuna training. The `Random_seed` is set to `None` because there are multiple simulations. If you set it to a number, it will choose a fixed sample for each sampler. The `Samplers` key contains the list of samplers to use. The `Hyperparameter Range` contains the range for simulating the L1 and L2 hyperparameters.

   3.4. **Other Parameters**: The rest of the parameters are automatically set. Run this file and check the `Outputs/results_summary/` folder for the final results.
